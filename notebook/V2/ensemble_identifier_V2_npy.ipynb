{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "from model import BertCustomBinaryClassifier\n",
    "from utils.ensemble_utils import make_predictions\n",
    "from utils.evaluate_metrics import evaluate_metrics\n",
    "from utils.data_preprocessing import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transforkmer_values.modeling_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"\")\n",
    "parser.add_argument(\"--max_length\", type=int, default=200, help=\"\")\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BERT Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_features(model, dataloader, kmer=3):\n",
    "    \"\"\"\n",
    "    Perform forward pass through the BERT model and retrieve the 768-dimensional features\n",
    "    (averaged token embeddings after excluding [CLS] and [SEP]).\n",
    "\n",
    "    Args:\n",
    "        model (BertCustomBinaryClassifier): The BERT-based model.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing batches of data.\n",
    "        kmer (int, optional): The number of trailing tokens to exclude. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The extracted 768-dimensional features for each input in the dataloader.\n",
    "    \"\"\"\n",
    "    # Set the device to GPU if available, otherwise use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move the model to the device\n",
    "\n",
    "    # Initialize a list to store the extracted features\n",
    "    all_features = []\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        for batch in dataloader:\n",
    "            # Move input data to the specified device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # Pass input through the BERT model\n",
    "            bert_outputs = model.bert(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Extract hidden states from the last layer\n",
    "            last_hidden_states = bert_outputs.last_hidden_state\n",
    "\n",
    "            # Exclude [CLS] and [SEP]\n",
    "            sequence_length = last_hidden_states.size(1)\n",
    "            start_index = 1  # Exclude [CLS] token\n",
    "            end_index = sequence_length - kmer # Exclude [SEP] token\n",
    "\n",
    "            if end_index > start_index:\n",
    "                token_embeddings = last_hidden_states[:, start_index:end_index] # Shape: (batch_size, num_tokens, hidden_size)\n",
    "                averaged_embeddings = token_embeddings.mean(dim=-2) # Shape: (batch_size, hidden_size)\n",
    "            else:\n",
    "                # Handle cases where sequence length is too short\n",
    "                averaged_embeddings = torch.zeros(last_hidden_states.size(0), 768).to(last_hidden_states.device)\n",
    "\n",
    "            # Move the features to CPU and convert to numpy array\n",
    "            all_features.append(averaged_embeddings.cpu().numpy())\n",
    "\n",
    "    # Concatenate the features from all batches and return\n",
    "    return np.concatenate(all_features, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.5\n",
      "Identifier model date: 2025-02-27_V2\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.50\n",
    "kmer_values = [3, 4, 5, 6]\n",
    "model_date = \"2025-02-27_V2\"\n",
    "\n",
    "results = []  # List to store results\n",
    "train_predictions_list, test_predictions_list = [], []  # Lists for storing model predictions\n",
    "train_labels_list, test_labels_list = [], []  # Lists for storing true labels\n",
    "train_logits_list, test_logits_list = [], [] # Lists to store logits\n",
    "\n",
    "print(f\"Threshold: {threshold}\")\n",
    "print(f\"Identifier model date: {model_date}\")\n",
    "\n",
    "for kmer in kmer_values:\n",
    "\n",
    "    args.model_path = f\"./outputs/identifier_models/{model_date}/{kmer}-mer\"\n",
    "    args.test_data_path = f\"./data/enhancer_identification/{kmer}-mer_identification_test.txt\"\n",
    "    args.train_data_path = f\"./data/enhancer_identification/{kmer}-mer_identification_train.txt\"\n",
    "\n",
    "    # Load training and test datasets\n",
    "    train_dataset = load_dataset(args, validation=False)\n",
    "    test_dataset = load_dataset(args, validation=True)\n",
    "\n",
    "    # Initialize data loaders for batch processing\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = BertCustomBinaryClassifier.from_pretrained(args.model_path, num_labels=1).to(device)\n",
    "\n",
    "    # Prediction on training datasets\n",
    "    train_predictions, train_labels = make_predictions(model, train_dataloader, kmer=kmer)\n",
    "    train_predictions_list.append(train_predictions)\n",
    "    train_labels_list.append(train_labels)\n",
    "\n",
    "    # Training logits\n",
    "    train_logits = get_bert_features(model, train_dataloader, kmer=kmer)\n",
    "    train_logits_list.append(train_logits) # Store the logits\n",
    "\n",
    "\n",
    "    acc, sn, sp, mcc, auc = evaluate_metrics(train_predictions, train_labels)\n",
    "    results.append({\"k-mer\": kmer, \"Dataset\": \"Train\", \"Accuracy\": acc, \"Sensitivity\": sn, \"Specificity\": sp, \"MCC\": mcc, \"AUC\": auc})\n",
    "\n",
    "    # Prediction on test (independent) dataset\n",
    "    test_predictions, test_labels = make_predictions(model, test_dataloader, kmer=kmer)\n",
    "    test_predictions_list.append(test_predictions)\n",
    "    test_labels_list.append(test_labels)\n",
    "\n",
    "    \n",
    "\n",
    "    # Testing logits\n",
    "    test_logits = get_bert_features(model, test_dataloader, kmer=kmer)\n",
    "    test_logits_list.append(test_logits) # Store the logits\n",
    "    \n",
    "    acc, sn, sp, mcc, auc = evaluate_metrics(test_predictions, test_labels)\n",
    "    results.append({\"k-mer\": kmer, \"Dataset\": \"Test\", \"Accuracy\": acc, \"Sensitivity\": sn, \"Specificity\": sp, \"MCC\": mcc, \"AUC\": auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_predictions_list: [2968, 2968, 2968, 2968]\n",
      "Shape of test_predictions_list: [400, 400, 400, 400]\n",
      "Shape of train_labels_list: [2968, 2968, 2968, 2968]\n",
      "Shape of test_labels_list: [400, 400, 400, 400]\n",
      "Shape of train_logits_list: [(2968, 768), (2968, 768), (2968, 768), (2968, 768)]\n",
      "Shape of test_logits_list: [(400, 768), (400, 768), (400, 768), (400, 768)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_predictions_list:\", [len(item) for item in train_predictions_list])\n",
    "print(\"Shape of test_predictions_list:\", [len(item) for item in test_predictions_list])\n",
    "print(\"Shape of train_labels_list:\", [len(item) for item in train_labels_list])\n",
    "print(\"Shape of test_labels_list:\", [len(item) for item in test_labels_list])\n",
    "print(\"Shape of train_logits_list:\", [item.shape for item in train_logits_list])\n",
    "print(\"Shape of test_logits_list:\", [item.shape for item in test_logits_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All lists have been saved to .npy files in the 'outputs/npy' directory.\n"
     ]
    }
   ],
   "source": [
    "output_root_dir = \"outputs\"\n",
    "npy_subfolder = \"npy\"\n",
    "output_npy_dir = os.path.join(output_root_dir, npy_subfolder)\n",
    "os.makedirs(output_npy_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(output_npy_dir, \"train_predictions.npy\"), np.array(train_predictions_list))\n",
    "np.save(os.path.join(output_npy_dir, \"test_predictions.npy\"), np.array(test_predictions_list))\n",
    "np.save(os.path.join(output_npy_dir, \"train_labels.npy\"), np.array(train_labels_list))\n",
    "np.save(os.path.join(output_npy_dir, \"test_labels.npy\"), np.array(test_labels_list))\n",
    "np.save(os.path.join(output_npy_dir, \"train_logits.npy\"), np.array(train_logits_list))\n",
    "np.save(os.path.join(output_npy_dir, \"test_logits.npy\"), np.array(test_logits_list))\n",
    "\n",
    "print(f\"All lists have been saved to .npy files in the '{output_npy_dir}' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
