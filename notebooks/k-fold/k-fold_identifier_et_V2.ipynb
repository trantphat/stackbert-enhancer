{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import BertCustomBinaryClassifier\n",
    "from utils.ensemble_utils import make_predictions\n",
    "from utils.evaluate_metrics import evaluate_metrics\n",
    "from utils.data_preprocessing import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transforkmer_values.modeling_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"\")\n",
    "parser.add_argument(\"--max_length\", type=int, default=200, help=\"\")\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BERT Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.5\n",
      "Identifier model date: 2025-02-27_V2\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.50\n",
    "kmer_values = [3, 4, 5, 6]\n",
    "model_date = \"2025-02-27_V2\"\n",
    "\n",
    "results = []  # List to store results\n",
    "train_predictions_list, test_predictions_list = [], []  # Lists for storing model predictions\n",
    "train_labels_list, test_labels_list = [], []  # Lists for storing true labels\n",
    "\n",
    "print(f\"Threshold: {threshold}\")\n",
    "print(f\"Identifier model date: {model_date}\")\n",
    "\n",
    "for kmer in kmer_values:\n",
    "\n",
    "    args.model_path = f\"./outputs/identifier_models/{model_date}/{kmer}-mer\"\n",
    "    args.test_data_path = f\"./data/enhancer_identification/{kmer}-mer_identification_test.txt\"\n",
    "    args.train_data_path = f\"./data/enhancer_identification/{kmer}-mer_identification_train.txt\"\n",
    "\n",
    "    # Load training and test datasets\n",
    "    train_dataset = load_dataset(args, validation=False)\n",
    "    test_dataset = load_dataset(args, validation=True)\n",
    "\n",
    "    # Initialize data loaders for batch processing\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = BertCustomBinaryClassifier.from_pretrained(args.model_path, num_labels=1).to(device)\n",
    "\n",
    "    # Prediction on training datasets\n",
    "    train_predictions, train_labels = make_predictions(model, train_dataloader, kmer=kmer)\n",
    "    train_predictions_list.append(train_predictions)\n",
    "    train_labels_list.append(train_labels)\n",
    "\n",
    "    acc, sn, sp, mcc, auc = evaluate_metrics(train_predictions, train_labels)\n",
    "    results.append({\"k-mer\": kmer, \"Dataset\": \"Train\", \"Accuracy\": acc, \"Sensitivity\": sn, \"Specificity\": sp, \"MCC\": mcc, \"AUC\": auc})\n",
    "\n",
    "    # Prediction on test (independent) dataset\n",
    "    test_predictions, test_labels = make_predictions(model, test_dataloader, kmer=kmer)\n",
    "    test_predictions_list.append(test_predictions)\n",
    "    test_labels_list.append(test_labels)\n",
    "\n",
    "    acc, sn, sp, mcc, auc = evaluate_metrics(test_predictions, test_labels)\n",
    "    results.append({\"k-mer\": kmer, \"Dataset\": \"Test\", \"Accuracy\": acc, \"Sensitivity\": sn, \"Specificity\": sp, \"MCC\": mcc, \"AUC\": auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results:\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|   k-mer | Dataset   |   Accuracy |   Sensitivity |   Specificity |    MCC |    AUC |\n",
      "+=========+===========+============+===============+===============+========+========+\n",
      "|       3 | Train     |     0.8999 |        0.8477 |        0.9522 | 0.8043 | 0.9015 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|       4 | Train     |     0.9067 |        0.8504 |        0.9629 | 0.8185 | 0.9174 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|       5 | Train     |     0.8854 |        0.8194 |        0.9515 | 0.7777 | 0.9045 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|       6 | Train     |     0.8757 |        0.7978 |        0.9535 | 0.7606 | 0.8962 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "\n",
      "Test results:\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|   k-mer | Dataset   |   Accuracy |   Sensitivity |   Specificity |    MCC |    AUC |\n",
      "+=========+===========+============+===============+===============+========+========+\n",
      "|       3 | Test      |     0.7975 |        0.7600 |        0.8350 | 0.5967 | 0.8324 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|       4 | Test      |     0.8000 |        0.7100 |        0.8900 | 0.6100 | 0.8412 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|       5 | Test      |     0.8000 |        0.7550 |        0.8450 | 0.6024 | 0.8338 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|       6 | Test      |     0.7900 |        0.7200 |        0.8600 | 0.5858 | 0.8300 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results, columns=[\"k-mer\", \"Dataset\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"MCC\", \"AUC\"])\n",
    "\n",
    "# Split into training and test\n",
    "training_df = results_df[results_df['Dataset'].str.contains(\"Train\")]\n",
    "test_df = results_df[results_df['Dataset'].str.contains(\"Test\")]\n",
    "\n",
    "print(\"Training results:\")\n",
    "print(tabulate(training_df, headers=\"keys\", tablefmt=\"grid\", showindex=False, floatfmt=\".4f\"))\n",
    "\n",
    "print(\"\\nTest results:\")\n",
    "print(tabulate(test_df, headers=\"keys\", tablefmt=\"grid\", showindex=False, floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.4890\n",
      "Best Test Accuracy: 0.8275\n",
      "+----------------------------+------------+---------------+---------------+--------+--------+\n",
      "| Dataset                    |   Accuracy |   Sensitivity |   Specificity |    MCC |    AUC |\n",
      "+============================+============+===============+===============+========+========+\n",
      "| Weighted Ensemble Training |     0.9097 |        0.8612 |        0.9582 | 0.8233 | 0.9269 |\n",
      "+----------------------------+------------+---------------+---------------+--------+--------+\n",
      "| Weighted Ensemble Testing  |     0.8275 |        0.7650 |        0.8900 | 0.6602 | 0.8530 |\n",
      "+----------------------------+------------+---------------+---------------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "best_test_acc = 0.0\n",
    "best_threshold = 0.0\n",
    "\n",
    "# Weights for ensemble \n",
    "weights = np.array(results_df[results_df['Dataset'].str.contains(\"Train\")]['Accuracy'])\n",
    "\n",
    "# Normalize weights to ensure they sum to 1\n",
    "weights /= np.sum(weights)\n",
    "\n",
    "# Define threshold values to test\n",
    "threshold_values = np.arange(0.40, 0.81, 0.001)\n",
    "\n",
    "# Variables to store the best metrics\n",
    "best_train_acc, best_train_sn, best_train_sp, best_train_mcc, best_train_auc = 0, 0, 0, 0, 0\n",
    "best_test_acc, best_test_sn, best_test_sp, best_test_mcc, best_test_auc = 0, 0, 0, 0, 0\n",
    "\n",
    "# Loop through threshold values\n",
    "for threshold in threshold_values:\n",
    "    # Weighted average for training predictions\n",
    "    train_predictions_weighted = np.average(train_predictions_list, axis=0, weights=weights)\n",
    "    train_labels_average = np.array(train_labels_list).mean(axis=0)\n",
    "    \n",
    "    # Evaluate training metrics\n",
    "    train_acc, train_sn, train_sp, train_mcc, train_auc = evaluate_metrics(\n",
    "        train_predictions_weighted, train_labels_average, threshold=threshold\n",
    "    )\n",
    "\n",
    "    # Weighted average for test predictions\n",
    "    test_predictions_weighted = np.average(test_predictions_list, axis=0, weights=weights)\n",
    "    test_labels_average = np.array(test_labels_list).mean(axis=0)\n",
    "\n",
    "    # Evaluate test metrics\n",
    "    test_acc, test_sn, test_sp, test_mcc, test_auc = evaluate_metrics(\n",
    "        test_predictions_weighted, test_labels_average, threshold=threshold\n",
    "    )\n",
    "    \n",
    "    # Check if current threshold yields the best test accuracy or compare MCC when accuracies are equal\n",
    "    if (test_acc > best_test_acc) or (test_acc == best_test_acc and train_acc > best_train_acc):\n",
    "        best_test_acc = test_acc\n",
    "        best_threshold = threshold\n",
    "\n",
    "        # Store the best metrics\n",
    "        best_train_acc, best_train_sn, best_train_sp, best_train_mcc, best_train_auc = train_acc, train_sn, train_sp, train_mcc, train_auc\n",
    "        best_test_acc, best_test_sn, best_test_sp, best_test_mcc, best_test_auc = test_acc, test_sn, test_sp, test_mcc, test_auc\n",
    "\n",
    "# Print the best threshold and corresponding test accuracy\n",
    "print(f\"Best Threshold: {best_threshold:.4f}\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f}\")\n",
    "\n",
    "# Update results table with the best weighted ensemble metrics\n",
    "ensemble_results_weighted = [\n",
    "    [\"Weighted Ensemble Training\", f\"{best_train_acc:.4f}\", f\"{best_train_sn:.4f}\", f\"{best_train_sp:.4f}\", f\"{best_train_mcc:.4f}\", f\"{best_train_auc:.4f}\"],\n",
    "    [\"Weighted Ensemble Testing\", f\"{best_test_acc:.4f}\", f\"{best_test_sn:.4f}\", f\"{best_test_sp:.4f}\", f\"{best_test_mcc:.4f}\", f\"{best_test_auc:.4f}\"]\n",
    "]\n",
    "ensemble_results_df_weighted = pd.DataFrame(ensemble_results_weighted, columns=[\"Dataset\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"MCC\", \"AUC\"])\n",
    "\n",
    "# Display results\n",
    "print(tabulate(ensemble_results_df_weighted, headers=\"keys\", tablefmt=\"grid\", showindex=False, floatfmt=\".4f\"))\n",
    "threshold = best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|   Sample |      3 |      4 |      5 |      6 |   Ensemble |   Label |   Prediction | Correct   |\n",
      "+==========+========+========+========+========+============+=========+==============+===========+\n",
      "|        1 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        2 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        3 | 0.2678 | 0.1379 | 0.8363 | 0.7124 |     0.4886 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        4 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        5 | 0.2677 | 0.1379 | 0.1563 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        6 | 0.2676 | 0.1379 | 0.1561 | 0.2841 |     0.2114 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        7 | 0.2676 | 0.8544 | 0.1563 | 0.7124 |     0.4976 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        8 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        9 | 0.7273 | 0.1390 | 0.1561 | 0.2834 |     0.3265 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       10 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       11 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       12 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       13 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       14 | 0.2736 | 0.1380 | 0.1561 | 0.2862 |     0.2135 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       15 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       16 | 0.2676 | 0.1379 | 0.1562 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       17 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       18 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       19 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       20 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       21 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       22 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       23 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       24 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       25 | 0.7252 | 0.1379 | 0.1562 | 0.2834 |     0.3257 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       26 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       27 | 0.7274 | 0.8544 | 0.8364 | 0.7123 |     0.7826 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       28 | 0.2677 | 0.1384 | 0.1561 | 0.2834 |     0.2114 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       29 | 0.2720 | 0.1379 | 0.1561 | 0.2834 |     0.2124 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       30 | 0.7272 | 0.3248 | 0.8362 | 0.2836 |     0.5430 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       31 | 0.2678 | 0.1379 | 0.1561 | 0.2835 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       32 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       33 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       34 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       35 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       36 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       37 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       38 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       39 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       40 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       41 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       42 | 0.2677 | 0.1379 | 0.8362 | 0.2834 |     0.3813 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       43 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       44 | 0.7273 | 0.1380 | 0.1561 | 0.3456 |     0.3417 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       45 | 0.2676 | 0.1379 | 0.1562 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       46 | 0.2839 | 0.1379 | 0.1561 | 0.2834 |     0.2153 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       47 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       48 | 0.7274 | 0.1409 | 0.1600 | 0.2840 |     0.3281 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       49 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       50 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       51 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       52 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       53 | 0.2676 | 0.1380 | 0.1561 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       54 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       55 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       56 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       57 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       58 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       59 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       60 | 0.5295 | 0.1379 | 0.8362 | 0.2834 |     0.4468 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       61 | 0.2676 | 0.1379 | 0.1562 | 0.2835 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       62 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       63 | 0.7190 | 0.1867 | 0.1563 | 0.2834 |     0.3363 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       64 | 0.2687 | 0.1521 | 0.8350 | 0.7119 |     0.4919 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       65 | 0.7258 | 0.1421 | 0.8363 | 0.7124 |     0.6041 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       66 | 0.7274 | 0.1558 | 0.8357 | 0.7124 |     0.6078 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       67 | 0.2677 | 0.1379 | 0.1561 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       68 | 0.2677 | 0.8544 | 0.1580 | 0.2834 |     0.3909 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       69 | 0.7274 | 0.1381 | 0.8364 | 0.7124 |     0.6036 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       70 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       71 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       72 | 0.2678 | 0.1379 | 0.1562 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       73 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       74 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       75 | 0.2677 | 0.8544 | 0.1592 | 0.2868 |     0.3920 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       76 | 0.2676 | 0.1379 | 0.1562 | 0.6385 |     0.3000 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       77 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       78 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       79 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       80 | 0.2676 | 0.1379 | 0.1562 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       81 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       82 | 0.2676 | 0.1379 | 0.1574 | 0.2834 |     0.2116 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       83 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       84 | 0.2825 | 0.1379 | 0.1562 | 0.2835 |     0.2150 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       85 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       86 | 0.2677 | 0.1379 | 0.1584 | 0.2835 |     0.2119 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       87 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       88 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       89 | 0.2676 | 0.1379 | 0.1566 | 0.2834 |     0.2114 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       90 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       91 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       92 | 0.2677 | 0.1379 | 0.1561 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       93 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       94 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       95 | 0.2705 | 0.1379 | 0.1562 | 0.2835 |     0.2120 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       96 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       97 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       98 | 0.2676 | 0.1379 | 0.1564 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       99 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      100 | 0.2677 | 0.8544 | 0.1562 | 0.2834 |     0.3904 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      101 | 0.2676 | 0.1379 | 0.8363 | 0.2834 |     0.3813 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      102 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      103 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      104 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      105 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      106 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      107 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      108 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      109 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      110 | 0.7274 | 0.1380 | 0.1716 | 0.2835 |     0.3301 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      111 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      112 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      113 | 0.2676 | 0.1387 | 0.8341 | 0.7124 |     0.4882 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      114 | 0.2676 | 0.1385 | 0.1631 | 0.2834 |     0.2132 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      115 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      116 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      117 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      118 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      119 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      120 | 0.2699 | 0.1381 | 0.8355 | 0.7122 |     0.4889 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      121 | 0.2682 | 0.8544 | 0.1561 | 0.7120 |     0.4977 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      122 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      123 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      124 | 0.2676 | 0.1379 | 0.1562 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      125 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      126 | 0.7273 | 0.8544 | 0.8356 | 0.2834 |     0.6752 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      127 | 0.2680 | 0.1379 | 0.1562 | 0.2834 |     0.2114 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      128 | 0.2676 | 0.1823 | 0.1567 | 0.7124 |     0.3298 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      129 | 0.2689 | 0.1380 | 0.7962 | 0.2834 |     0.3716 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      130 | 0.2677 | 0.1379 | 0.1561 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      131 | 0.2678 | 0.8543 | 0.8363 | 0.5861 |     0.6361 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      132 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      133 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      134 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      135 | 0.2683 | 0.1379 | 0.1561 | 0.2834 |     0.2114 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      136 | 0.2751 | 0.6824 | 0.1870 | 0.7124 |     0.4642 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      137 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      138 | 0.7273 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      139 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      140 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      141 | 0.7271 | 0.1379 | 0.1563 | 0.2837 |     0.3262 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      142 | 0.2676 | 0.1379 | 0.1562 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      143 | 0.2676 | 0.1393 | 0.1562 | 0.2834 |     0.2116 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      144 | 0.7273 | 0.1379 | 0.1563 | 0.2834 |     0.3262 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      145 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      146 | 0.7273 | 0.8544 | 0.8364 | 0.7122 |     0.7826 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      147 | 0.2690 | 0.1414 | 0.1564 | 0.2946 |     0.2153 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      148 | 0.2677 | 0.1379 | 0.1574 | 0.2834 |     0.2116 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      149 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      150 | 0.2676 | 0.8544 | 0.8359 | 0.7123 |     0.6675 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      151 | 0.2676 | 0.1379 | 0.1563 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      152 | 0.7271 | 0.1381 | 0.1571 | 0.2837 |     0.3265 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      153 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      154 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      155 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      156 | 0.2721 | 0.1400 | 0.1761 | 0.2834 |     0.2179 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      157 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      158 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      159 | 0.2700 | 0.1379 | 0.1564 | 0.2834 |     0.2119 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      160 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      161 | 0.2677 | 0.1379 | 0.1561 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      162 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      163 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      164 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      165 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      166 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      167 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      168 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      169 | 0.2678 | 0.1379 | 0.3190 | 0.2834 |     0.2520 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      170 | 0.7273 | 0.1380 | 0.1648 | 0.2834 |     0.3284 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      171 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      172 | 0.2679 | 0.8544 | 0.8364 | 0.7124 |     0.6678 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      173 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      174 | 0.2676 | 0.1379 | 0.8348 | 0.4108 |     0.4128 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      175 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      176 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      177 | 0.7268 | 0.1379 | 0.2501 | 0.2834 |     0.3495 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      178 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      179 | 0.7274 | 0.8482 | 0.8364 | 0.3355 |     0.6869 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      180 | 0.7273 | 0.1379 | 0.1563 | 0.3453 |     0.3417 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      181 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      182 | 0.2677 | 0.1379 | 0.6097 | 0.2834 |     0.3247 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      183 | 0.2678 | 0.1379 | 0.8346 | 0.2834 |     0.3809 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      184 | 0.2676 | 0.1379 | 0.1562 | 0.2834 |     0.2113 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      185 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      186 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      187 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      188 | 0.7273 | 0.1380 | 0.1569 | 0.7122 |     0.4336 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      189 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      190 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      191 | 0.7274 | 0.1380 | 0.1808 | 0.2836 |     0.3324 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      192 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      193 | 0.7273 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      194 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      195 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      196 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      197 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      198 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      199 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      200 | 0.7273 | 0.1379 | 0.1562 | 0.6161 |     0.4094 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      201 | 0.7274 | 0.8543 | 0.8362 | 0.7123 |     0.7825 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      202 | 0.7273 | 0.8544 | 0.8360 | 0.7124 |     0.7825 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      203 | 0.7274 | 0.8544 | 0.8362 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      204 | 0.7274 | 0.8544 | 0.8364 | 0.7123 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      205 | 0.7273 | 0.8544 | 0.8362 | 0.7123 |     0.7825 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      206 | 0.7273 | 0.8544 | 0.8361 | 0.7124 |     0.7825 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      207 | 0.7274 | 0.8544 | 0.8364 | 0.6889 |     0.7768 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      208 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      209 | 0.7273 | 0.8543 | 0.8355 | 0.7077 |     0.7812 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      210 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      211 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      212 | 0.7273 | 0.8544 | 0.8361 | 0.7124 |     0.7825 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      213 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      214 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      215 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      216 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      217 | 0.7274 | 0.8544 | 0.8364 | 0.7123 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      218 | 0.2676 | 0.7660 | 0.8354 | 0.7124 |     0.6454 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      219 | 0.7274 | 0.8543 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      220 | 0.7274 | 0.8544 | 0.8186 | 0.7124 |     0.7782 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      221 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      222 | 0.7273 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      223 | 0.7258 | 0.8544 | 0.8362 | 0.7122 |     0.7821 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      224 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2113 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      225 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      226 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      227 | 0.2676 | 0.8544 | 0.8357 | 0.2834 |     0.5603 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      228 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      229 | 0.7273 | 0.8544 | 0.8362 | 0.2956 |     0.6784 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      230 | 0.7084 | 0.8543 | 0.8360 | 0.7117 |     0.7776 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      231 | 0.7272 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      232 | 0.2678 | 0.1379 | 0.8362 | 0.2834 |     0.3813 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      233 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      234 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      235 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      236 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      237 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      238 | 0.7268 | 0.8544 | 0.8363 | 0.7124 |     0.7825 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      239 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      240 | 0.7273 | 0.8544 | 0.8361 | 0.7119 |     0.7824 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      241 | 0.7273 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      242 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      243 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      244 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      245 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      246 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      247 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      248 | 0.7273 | 0.8544 | 0.7266 | 0.2834 |     0.6479 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      249 | 0.7272 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      250 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      251 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      252 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      253 | 0.2678 | 0.1379 | 0.1562 | 0.2834 |     0.2113 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      254 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      255 | 0.7274 | 0.8536 | 0.8362 | 0.7124 |     0.7824 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      256 | 0.7274 | 0.8542 | 0.8363 | 0.7123 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      257 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      258 | 0.7271 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      259 | 0.7273 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      260 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      261 | 0.7272 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      262 | 0.7273 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      263 | 0.7273 | 0.1379 | 0.8359 | 0.7041 |     0.6013 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      264 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      265 | 0.7274 | 0.2240 | 0.8361 | 0.7124 |     0.6250 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      266 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      267 | 0.7273 | 0.8544 | 0.1563 | 0.7121 |     0.6125 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      268 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      269 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      270 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      271 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      272 | 0.2711 | 0.8544 | 0.1562 | 0.2834 |     0.3913 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      273 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      274 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      275 | 0.7273 | 0.8544 | 0.2154 | 0.2835 |     0.5201 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      276 | 0.7273 | 0.8487 | 0.1563 | 0.7123 |     0.6112 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      277 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      278 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      279 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      280 | 0.2677 | 0.8544 | 0.7219 | 0.6534 |     0.6243 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      281 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      282 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      283 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      284 | 0.7274 | 0.8544 | 0.8363 | 0.7123 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      285 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      286 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      287 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      288 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      289 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      290 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      291 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      292 | 0.7273 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      293 | 0.7274 | 0.8544 | 0.8360 | 0.7124 |     0.7825 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      294 | 0.7274 | 0.8544 | 0.8295 | 0.2834 |     0.6737 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      295 | 0.7274 | 0.8543 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      296 | 0.7274 | 0.8544 | 0.8355 | 0.7124 |     0.7824 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      297 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      298 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      299 | 0.7273 | 0.8544 | 0.8363 | 0.7123 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      300 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      301 | 0.2677 | 0.1379 | 0.1562 | 0.2834 |     0.2113 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      302 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      303 | 0.2676 | 0.1379 | 0.1588 | 0.2834 |     0.2119 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      304 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      305 | 0.7274 | 0.1393 | 0.8364 | 0.2836 |     0.4967 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      306 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      307 | 0.2676 | 0.1379 | 0.1583 | 0.2834 |     0.2118 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      308 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      309 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      310 | 0.7273 | 0.1379 | 0.1577 | 0.2834 |     0.3266 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      311 | 0.4807 | 0.8544 | 0.8364 | 0.7124 |     0.7210 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      312 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      313 | 0.7272 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      314 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      315 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      316 | 0.7274 | 0.1919 | 0.8354 | 0.7124 |     0.6168 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      317 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      318 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      319 | 0.7273 | 0.8544 | 0.8361 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      320 | 0.7270 | 0.8540 | 0.1573 | 0.3699 |     0.5271 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      321 | 0.7272 | 0.8544 | 0.7968 | 0.7124 |     0.7727 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      322 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      323 | 0.7273 | 0.1379 | 0.8362 | 0.7124 |     0.6035 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      324 | 0.2677 | 0.1379 | 0.1774 | 0.7122 |     0.3238 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      325 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      326 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      327 | 0.2681 | 0.8544 | 0.8362 | 0.7122 |     0.6677 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      328 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      329 | 0.7274 | 0.8544 | 0.8358 | 0.7124 |     0.7825 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      330 | 0.7274 | 0.1379 | 0.8358 | 0.7119 |     0.6033 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      331 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      332 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      333 | 0.7274 | 0.8544 | 0.8355 | 0.7123 |     0.7824 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      334 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      335 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      336 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      337 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      338 | 0.2676 | 0.1380 | 0.1567 | 0.2834 |     0.2114 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      339 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      340 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      341 | 0.7274 | 0.8525 | 0.8364 | 0.7124 |     0.7822 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      342 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2113 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      343 | 0.7268 | 0.8058 | 0.8314 | 0.2834 |     0.6618 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      344 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      345 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      346 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      347 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      348 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      349 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      350 | 0.7181 | 0.1379 | 0.8362 | 0.7123 |     0.6011 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      351 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      352 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      353 | 0.7272 | 0.8544 | 0.8357 | 0.7124 |     0.7824 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      354 | 0.7274 | 0.8544 | 0.8363 | 0.2837 |     0.6755 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      355 | 0.6771 | 0.2017 | 0.8364 | 0.7124 |     0.6069 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      356 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      357 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      358 | 0.2676 | 0.1379 | 0.1563 | 0.2834 |     0.2113 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      359 | 0.2676 | 0.1379 | 0.8350 | 0.2834 |     0.3810 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      360 | 0.7274 | 0.7708 | 0.8364 | 0.7123 |     0.7617 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      361 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      362 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      363 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      364 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      365 | 0.7273 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      366 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      367 | 0.7259 | 0.8544 | 0.8358 | 0.6992 |     0.7788 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      368 | 0.7274 | 0.1380 | 0.8361 | 0.7123 |     0.6034 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      369 | 0.7274 | 0.8544 | 0.8354 | 0.7124 |     0.7824 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      370 | 0.7273 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      371 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      372 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      373 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      374 | 0.7246 | 0.1379 | 0.1561 | 0.2834 |     0.3255 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      375 | 0.7274 | 0.8544 | 0.8361 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      376 | 0.7274 | 0.8335 | 0.8363 | 0.7124 |     0.7774 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      377 | 0.2676 | 0.1379 | 0.1564 | 0.2834 |     0.2113 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      378 | 0.7274 | 0.8544 | 0.8326 | 0.7124 |     0.7817 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      379 | 0.7273 | 0.1526 | 0.8364 | 0.7124 |     0.6072 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      380 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      381 | 0.7265 | 0.1384 | 0.8325 | 0.2834 |     0.4952 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      382 | 0.2676 | 0.1379 | 0.1606 | 0.2834 |     0.2124 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      383 | 0.7274 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      384 | 0.7273 | 0.8544 | 0.8363 | 0.7124 |     0.7826 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      385 | 0.2700 | 0.1379 | 0.1562 | 0.2834 |     0.2119 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      386 | 0.2688 | 0.1379 | 0.1581 | 0.2834 |     0.2121 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      387 | 0.7274 | 0.8541 | 0.8306 | 0.2856 |     0.6744 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      388 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      389 | 0.2676 | 0.1379 | 0.1568 | 0.2834 |     0.2114 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      390 | 0.2690 | 0.1379 | 0.1561 | 0.2834 |     0.2116 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      391 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      392 | 0.2676 | 0.1379 | 0.8340 | 0.2834 |     0.3807 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      393 | 0.7273 | 0.1379 | 0.1562 | 0.7122 |     0.4334 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      394 | 0.7273 | 0.3805 | 0.1607 | 0.7124 |     0.4952 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      395 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      396 | 0.7270 | 0.1379 | 0.1562 | 0.2835 |     0.3261 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      397 | 0.7274 | 0.8544 | 0.8364 | 0.7124 |     0.7827 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      398 | 0.2675 | 0.1379 | 0.1561 | 0.2834 |     0.2112 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      399 | 0.2676 | 0.1379 | 0.1561 | 0.2834 |     0.2113 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      400 | 0.2676 | 0.1379 | 0.1562 | 0.2838 |     0.2114 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate individual model scores for each sample\n",
    "individual_model_scores = []\n",
    "for i, kmer in enumerate(kmer_values):\n",
    "    for sample_idx in range(len(test_predictions_list[i])):\n",
    "        individual_model_scores.append({\n",
    "            \"Sample\": sample_idx + 1,\n",
    "            \"k-mer\": kmer,\n",
    "            \"Score\": test_predictions_list[i][sample_idx]\n",
    "        })\n",
    "\n",
    "# Calculate ensemble model scores\n",
    "ensemble_test_predictions = np.array(test_predictions_list).mean(axis=0)\n",
    "for sample_idx in range(len(ensemble_test_predictions)):\n",
    "    individual_model_scores.append({\n",
    "        \"Sample\": sample_idx + 1,\n",
    "        \"k-mer\": \"Ensemble\",\n",
    "        \"Score\": ensemble_test_predictions[sample_idx]\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "individual_model_scores_df = pd.DataFrame(individual_model_scores)\n",
    "\n",
    "# Reshape DataFrame to have each sample as a row, and each model as a column\n",
    "pivot_df = individual_model_scores_df.pivot(index=\"Sample\", columns=\"k-mer\", values=\"Score\")\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "# Adding 'Label' and 'Prediction' columns\n",
    "pivot_df[\"Label\"] = test_labels_list[0]  \n",
    "pivot_df[\"Prediction\"] = (pivot_df[\"Ensemble\"] >= threshold).astype(int)  # Converting ensemble scores to binary predictions\n",
    "\n",
    "# Rename columns to match desired format\n",
    "# Assuming you have 4 k-mer values in kmer_values list\n",
    "column_mapping = {kmer: str(i + 3) for i, kmer in enumerate(kmer_values)}\n",
    "pivot_df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Add 'Correct' column\n",
    "pivot_df[\"Correct\"] = pivot_df[\"Label\"] == pivot_df[\"Prediction\"]\n",
    "\n",
    "# Display the table\n",
    "print(tabulate(pivot_df, headers=\"keys\", tablefmt=\"grid\", showindex=False, floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = round(threshold, 4)\n",
    "print(f\"Threshold: {threshold}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct  False  True \n",
      "Label                \n",
      "0           23    177\n",
      "1           47    153\n",
      "\n",
      "Combined incorrect predictions counts:\n",
      "+---------+-----------------------------------+-----------------------------------+-------+\n",
      "| Model   |   Incorrect Predictions (Label 0) |   Incorrect Predictions (Label 1) |   Sum |\n",
      "+=========+===================================+===================================+=======+\n",
      "| 3       |                                33 |                                48 |    81 |\n",
      "+---------+-----------------------------------+-----------------------------------+-------+\n",
      "| 4       |                                22 |                                58 |    80 |\n",
      "+---------+-----------------------------------+-----------------------------------+-------+\n",
      "| 5       |                                31 |                                49 |    80 |\n",
      "+---------+-----------------------------------+-----------------------------------+-------+\n",
      "| 6       |                                28 |                                56 |    84 |\n",
      "+---------+-----------------------------------+-----------------------------------+-------+\n",
      "| Sum     |                               114 |                               211 |   325 |\n",
      "+---------+-----------------------------------+-----------------------------------+-------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate counts of correct and incorrect predictions for each label\n",
    "label_counts = pivot_df.groupby([\"Label\", \"Correct\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Display the counts\n",
    "print(label_counts)\n",
    "\n",
    "# Calculate incorrect predictions for each k-mer model\n",
    "incorrect_counts = pivot_df[[\"Label\"]].copy()\n",
    "for kmer, col_name in column_mapping.items():\n",
    "    incorrect_counts[col_name] = pivot_df[\"Label\"] != (pivot_df[col_name] >= threshold).astype(int)\n",
    "\n",
    "# Split DataFrame into two separate DataFrames based on labels\n",
    "label_0_df = pivot_df[pivot_df[\"Label\"] == 0]\n",
    "label_1_df = pivot_df[pivot_df[\"Label\"] == 1]\n",
    "\n",
    "# Calculate incorrect predictions for each k-mer model for both labels separately\n",
    "incorrect_counts_label_0 = label_0_df[[\"Label\"]].copy()\n",
    "incorrect_counts_label_1 = label_1_df[[\"Label\"]].copy()\n",
    "\n",
    "for kmer, col_name in column_mapping.items():\n",
    "    incorrect_counts_label_0[col_name] = label_0_df[\"Label\"] != (label_0_df[col_name] >= threshold).astype(int)\n",
    "    incorrect_counts_label_1[col_name] = label_1_df[\"Label\"] != (label_1_df[col_name] >= threshold).astype(int)\n",
    "\n",
    "# Sum incorrect predictions for each k-mer model for both labels separately\n",
    "incorrect_counts_sum_label_0 = incorrect_counts_label_0.sum(axis=0)[1:]\n",
    "incorrect_counts_sum_label_1 = incorrect_counts_label_1.sum(axis=0)[1:]\n",
    "\n",
    "# Combine the incorrect predictions into a single DataFrame\n",
    "combined_incorrect_counts_df = pd.DataFrame({\n",
    "    \"Model\": list(incorrect_counts_sum_label_0.index),\n",
    "    \"Incorrect Predictions (Label 0)\": incorrect_counts_sum_label_0.values,\n",
    "    \"Incorrect Predictions (Label 1)\": incorrect_counts_sum_label_1.values\n",
    "})\n",
    "\n",
    "# Add a sum row at the end\n",
    "sum_row = pd.DataFrame({\n",
    "    \"Model\": [\"Sum\"],\n",
    "    \"Incorrect Predictions (Label 0)\": [incorrect_counts_sum_label_0.sum()],\n",
    "    \"Incorrect Predictions (Label 1)\": [incorrect_counts_sum_label_1.sum()]\n",
    "})\n",
    "\n",
    "combined_incorrect_counts_df = pd.concat([combined_incorrect_counts_df, sum_row], ignore_index=True)\n",
    "\n",
    "# Add a sum column for the rows\n",
    "combined_incorrect_counts_df[\"Sum\"] = combined_incorrect_counts_df[\"Incorrect Predictions (Label 0)\"] + combined_incorrect_counts_df[\"Incorrect Predictions (Label 1)\"]\n",
    "\n",
    "# Display the combined incorrect predictions counts\n",
    "print(\"\\nCombined incorrect predictions counts:\")\n",
    "print(tabulate(combined_incorrect_counts_df, headers=\"keys\", tablefmt=\"grid\", showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import  ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare meta-features for training and testing\n",
    "train_meta_features = np.column_stack(train_predictions_list)\n",
    "test_meta_features = np.column_stack(test_predictions_list)\n",
    "\n",
    "train_labels = train_labels_list[0]     \n",
    "test_labels = test_labels_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+---------------+---------------+--------+--------+\n",
      "| Dataset   |   Accuracy |   Sensitivity |   Specificity |    MCC |    AUC |\n",
      "+===========+============+===============+===============+========+========+\n",
      "| Train     |     1.0000 |        1.0000 |        1.0000 | 1.0000 | 1.0000 |\n",
      "+-----------+------------+---------------+---------------+--------+--------+\n",
      "| Test      |     0.8350 |        0.7500 |        0.9200 | 0.6799 | 0.8652 |\n",
      "+-----------+------------+---------------+---------------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=1000, random_state=42)\n",
    "\n",
    "threshold = 0.88   \n",
    "\n",
    "# Train the Stacking Classifier on the training set\n",
    "model.fit(train_meta_features, train_labels)\n",
    "\n",
    "# Meta-model predictions on the validation set\n",
    "train_predictions_et = model.predict_proba(train_meta_features)[:, 1]\n",
    "test_predictions_et = model.predict_proba(test_meta_features)[:, 1]\n",
    "\n",
    "train_acc, train_sn, train_sp, train_mcc, train_auc = evaluate_metrics(train_predictions_et, train_labels, threshold=threshold)\n",
    "test_acc, test_sn, test_sp, test_mcc, test_auc = evaluate_metrics(test_predictions_et, test_labels, threshold=threshold)\n",
    "\n",
    "# Display results in table using tabulate\n",
    "results = [[\"Train\", train_acc, train_sn, train_sp, train_mcc, train_auc], [\"Test\", test_acc, test_sn, test_sp, test_mcc, test_auc]]\n",
    "\n",
    "headers = [\"Dataset\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"MCC\", \"AUC\"]\n",
    "\n",
    "print(tabulate(results, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **K-Fold**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5-fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1/5\n",
      "Processing Fold 2/5\n",
      "Processing Fold 3/5\n",
      "Processing Fold 4/5\n",
      "Processing Fold 5/5\n",
      "\n",
      "5-fold cross-validation results:\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|   Fold |    ACC |     SN |     SP |    MCC |    AUC |\n",
      "+========+========+========+========+========+========+\n",
      "|      1 | 0.9024 | 0.8664 | 0.9371 | 0.8063 | 0.9259 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      2 | 0.9158 | 0.8824 | 0.9475 | 0.8328 | 0.9337 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      3 | 0.9293 | 0.8939 | 0.9682 | 0.8616 | 0.9453 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      4 | 0.9275 | 0.9094 | 0.9444 | 0.8551 | 0.9556 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      5 | 0.9123 | 0.8721 | 0.9549 | 0.8281 | 0.9311 |\n",
      "+--------+--------+--------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Parameters for k-fold\n",
    "k_folds = 5  # Number of folds\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "threshold = 0.5\n",
    "\n",
    "# Initialize results storage\n",
    "kfold_results = []\n",
    "\n",
    "# Convert meta-features and labels to numpy arrays for k-fold processing\n",
    "train_meta_features = np.array(train_meta_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_meta_features)):\n",
    "    print(f\"Processing Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    # Split data into training and validation sets for this fold\n",
    "    X_train, X_val = train_meta_features[train_idx], train_meta_features[val_idx]\n",
    "    y_train, y_val = train_labels[train_idx], train_labels[val_idx]\n",
    "\n",
    "    # Initialize and train Extra Trees model\n",
    "    model = ExtraTreesClassifier(n_estimators=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions on validation set\n",
    "    val_predictions = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Evaluate metrics\n",
    "    val_acc, val_sn, val_sp, val_mcc, val_auc = evaluate_metrics(val_predictions, y_val, threshold=threshold)\n",
    "\n",
    "    # Store results for this fold\n",
    "    kfold_results.append({\"Fold\": fold + 1, \"ACC\": val_acc, \"SN\": val_sn, \"SP\": val_sp, \"MCC\": val_mcc, \"AUC\": val_auc})\n",
    "\n",
    "# Display aggregated results across folds\n",
    "print(f'\\n{k_folds}-fold cross-validation results:')\n",
    "headers = [\"Fold\", \"ACC\", \"SN\", \"SP\", \"MCC\", \"AUC\"]\n",
    "results_table = [[result[\"Fold\"], result[\"ACC\"], result[\"SN\"], result[\"SP\"], result[\"MCC\"], result[\"AUC\"]] for result in kfold_results]\n",
    "print(tabulate(results_table, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 5-fold cross-validation results:\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "| Fold   |    ACC |     SN |     SP |    MCC |    AUC |\n",
      "+========+========+========+========+========+========+\n",
      "| Mean   | 0.9175 | 0.8848 | 0.9504 | 0.8368 | 0.9383 |\n",
      "+--------+--------+--------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display average results\n",
    "avg_acc = np.mean([result[\"ACC\"] for result in kfold_results])\n",
    "avg_sn = np.mean([result[\"SN\"] for result in kfold_results])\n",
    "avg_sp = np.mean([result[\"SP\"] for result in kfold_results])\n",
    "avg_mcc = np.mean([result[\"MCC\"] for result in kfold_results])\n",
    "avg_auc = np.mean([result[\"AUC\"] for result in kfold_results])\n",
    "\n",
    "print(f'Average {k_folds}-fold cross-validation results:')\n",
    "avg_results_table = [[\"Mean\", avg_acc, avg_sn, avg_sp, avg_mcc, avg_auc]]\n",
    "print(tabulate(avg_results_table, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10-fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1/10\n",
      "Processing Fold 2/10\n",
      "Processing Fold 3/10\n",
      "Processing Fold 4/10\n",
      "Processing Fold 5/10\n",
      "Processing Fold 6/10\n",
      "Processing Fold 7/10\n",
      "Processing Fold 8/10\n",
      "Processing Fold 9/10\n",
      "Processing Fold 10/10\n",
      "\n",
      "10-fold cross-validation results:\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|   Fold |    ACC |     SN |     SP |    MCC |    AUC |\n",
      "+========+========+========+========+========+========+\n",
      "|      1 | 0.9057 | 0.8581 | 0.9530 | 0.8150 | 0.9207 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      2 | 0.9057 | 0.8819 | 0.9281 | 0.8117 | 0.9405 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      3 | 0.9327 | 0.8919 | 0.9732 | 0.8681 | 0.9511 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      4 | 0.9024 | 0.8723 | 0.9295 | 0.8046 | 0.9178 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      5 | 0.9360 | 0.9026 | 0.9720 | 0.8746 | 0.9531 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      6 | 0.9226 | 0.8854 | 0.9643 | 0.8486 | 0.9315 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      7 | 0.9360 | 0.9220 | 0.9487 | 0.8718 | 0.9552 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      8 | 0.9192 | 0.8973 | 0.9404 | 0.8389 | 0.9542 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      9 | 0.9155 | 0.8865 | 0.9419 | 0.8313 | 0.9326 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|     10 | 0.9020 | 0.8476 | 0.9697 | 0.8129 | 0.9350 |\n",
      "+--------+--------+--------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Parameters for k-fold\n",
    "k_folds = 10  # Number of folds\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "threshold = 0.5\n",
    "\n",
    "# Initialize results storage\n",
    "kfold_results = []\n",
    "\n",
    "# Convert meta-features and labels to numpy arrays for k-fold processing\n",
    "train_meta_features = np.array(train_meta_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_meta_features)):\n",
    "    print(f\"Processing Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    # Split data into training and validation sets for this fold\n",
    "    X_train, X_val = train_meta_features[train_idx], train_meta_features[val_idx]\n",
    "    y_train, y_val = train_labels[train_idx], train_labels[val_idx]\n",
    "\n",
    "    # Initialize and train Extra Trees model\n",
    "    model = ExtraTreesClassifier(n_estimators=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions on validation set\n",
    "    val_predictions = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Evaluate metrics\n",
    "    val_acc, val_sn, val_sp, val_mcc, val_auc = evaluate_metrics(val_predictions, y_val, threshold=threshold)\n",
    "\n",
    "    # Store results for this fold\n",
    "    kfold_results.append({\"Fold\": fold + 1, \"ACC\": val_acc, \"SN\": val_sn, \"SP\": val_sp, \"MCC\": val_mcc, \"AUC\": val_auc})\n",
    "\n",
    "# Display aggregated results across folds\n",
    "print(f'\\n{k_folds}-fold cross-validation results:')\n",
    "headers = [\"Fold\", \"ACC\", \"SN\", \"SP\", \"MCC\", \"AUC\"]\n",
    "results_table = [[result[\"Fold\"], result[\"ACC\"], result[\"SN\"], result[\"SP\"], result[\"MCC\"], result[\"AUC\"]] for result in kfold_results]\n",
    "print(tabulate(results_table, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 10-fold cross-validation results:\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "| Fold   |    ACC |     SN |     SP |    MCC |    AUC |\n",
      "+========+========+========+========+========+========+\n",
      "| Mean   | 0.9178 | 0.8846 | 0.9521 | 0.8378 | 0.9392 |\n",
      "+--------+--------+--------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display average results\n",
    "avg_acc = np.mean([result[\"ACC\"] for result in kfold_results])\n",
    "avg_sn = np.mean([result[\"SN\"] for result in kfold_results])\n",
    "avg_sp = np.mean([result[\"SP\"] for result in kfold_results])\n",
    "avg_mcc = np.mean([result[\"MCC\"] for result in kfold_results])\n",
    "avg_auc = np.mean([result[\"AUC\"] for result in kfold_results])\n",
    "\n",
    "print(f'Average {k_folds}-fold cross-validation results:')\n",
    "avg_results_table = [[\"Mean\", avg_acc, avg_sn, avg_sp, avg_mcc, avg_auc]]\n",
    "print(tabulate(avg_results_table, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
