{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import BertCustomBinaryClassifier\n",
    "from utils.ensemble_utils import make_predictions\n",
    "from utils.evaluate_metrics import evaluate_metrics\n",
    "from utils.data_preprocessing import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transforkmer_values.modeling_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"\")\n",
    "parser.add_argument(\"--max_length\", type=int, default=200, help=\"\")\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BERT Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.5\n",
      "Classifier model date: 2025-02-27_V2\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.50\n",
    "kmer_values = [3, 4, 5, 6]\n",
    "model_date = \"2025-02-27_V2\"\n",
    "\n",
    "results = []  # List to store results\n",
    "train_predictions_list, test_predictions_list = [], []  # Lists for storing model predictions\n",
    "train_labels_list, test_labels_list = [], []  # Lists for storing true labels\n",
    "\n",
    "print(f\"Threshold: {threshold}\")\n",
    "print(f\"Classifier model date: {model_date}\")\n",
    "\n",
    "for kmer in kmer_values:\n",
    "\n",
    "    args.model_path = f\"./outputs/classifier_models/{model_date}/{kmer}-mer\"\n",
    "    args.test_data_path = f\"./data/enhancer_classification/{kmer}-mer_classification_test.txt\"\n",
    "    args.train_data_path = f\"./data/enhancer_classification/{kmer}-mer_classification_train.txt\"\n",
    "\n",
    "    # Load training and test datasets\n",
    "    train_dataset = load_dataset(args, validation=False)\n",
    "    test_dataset = load_dataset(args, validation=True)\n",
    "\n",
    "    # Initialize data loaders for batch processing\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = BertCustomBinaryClassifier.from_pretrained(args.model_path, num_labels=1).to(device)\n",
    "\n",
    "    # Prediction on training datasets\n",
    "    train_predictions, train_labels = make_predictions(model, train_dataloader, kmer=kmer)\n",
    "    train_predictions_list.append(train_predictions)\n",
    "    train_labels_list.append(train_labels)\n",
    "\n",
    "    acc, sn, sp, mcc, auc = evaluate_metrics(train_predictions, train_labels)\n",
    "    results.append({\"k-mer\": kmer, \"Dataset\": \"Train\", \"Accuracy\": acc, \"Sensitivity\": sn, \"Specificity\": sp, \"MCC\": mcc, \"AUC\": auc})\n",
    "\n",
    "    # Prediction on test (independent) dataset\n",
    "    test_predictions, test_labels = make_predictions(model, test_dataloader, kmer=kmer)\n",
    "    test_predictions_list.append(test_predictions)\n",
    "    test_labels_list.append(test_labels)\n",
    "\n",
    "    acc, sn, sp, mcc, auc = evaluate_metrics(test_predictions, test_labels)\n",
    "    results.append({\"k-mer\": kmer, \"Dataset\": \"Test\", \"Accuracy\": acc, \"Sensitivity\": sn, \"Specificity\": sp, \"MCC\": mcc, \"AUC\": auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training results:\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|   k-mer | Dataset   |   Accuracy |   Sensitivity |   Specificity |    MCC |    AUC |\n",
      "+=========+===========+============+===============+===============+========+========+\n",
      "|       3 | Train     |     0.8888 |        0.8302 |        0.9474 | 0.7830 | 0.9162 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|       4 | Train     |     0.9111 |        0.8639 |        0.9582 | 0.8258 | 0.9218 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|       5 | Train     |     0.8982 |        0.8329 |        0.9636 | 0.8034 | 0.8944 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|       6 | Train     |     0.9043 |        0.8423 |        0.9663 | 0.8149 | 0.8987 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "\n",
      "Test results:\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|   k-mer | Dataset   |   Accuracy |   Sensitivity |   Specificity |    MCC |    AUC |\n",
      "+=========+===========+============+===============+===============+========+========+\n",
      "|       3 | Test      |     0.8500 |        0.8400 |        0.8600 | 0.7001 | 0.8741 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|       4 | Test      |     0.8850 |        0.8800 |        0.8900 | 0.7700 | 0.8874 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|       5 | Test      |     0.8700 |        0.8400 |        0.9000 | 0.7413 | 0.8618 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n",
      "|       6 | Test      |     0.8550 |        0.8300 |        0.8800 | 0.7109 | 0.8678 |\n",
      "+---------+-----------+------------+---------------+---------------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results, columns=[\"k-mer\", \"Dataset\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"MCC\", \"AUC\"])\n",
    "\n",
    "# Split into training and test\n",
    "training_df = results_df[results_df['Dataset'].str.contains(\"Train\")]\n",
    "test_df = results_df[results_df['Dataset'].str.contains(\"Test\")]\n",
    "\n",
    "print(\"Training results:\")\n",
    "print(tabulate(training_df, headers=\"keys\", tablefmt=\"grid\", showindex=False, floatfmt=\".4f\"))\n",
    "\n",
    "print(\"\\nTest results:\")\n",
    "print(tabulate(test_df, headers=\"keys\", tablefmt=\"grid\", showindex=False, floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.4930\n",
      "Best Test Accuracy: 0.8850\n",
      "+----------------------------+------------+---------------+---------------+--------+--------+\n",
      "| Dataset                    |   Accuracy |   Sensitivity |   Specificity |    MCC |    AUC |\n",
      "+============================+============+===============+===============+========+========+\n",
      "| Weighted Ensemble Training |     0.9245 |        0.8706 |        0.9784 | 0.8540 | 0.9701 |\n",
      "+----------------------------+------------+---------------+---------------+--------+--------+\n",
      "| Weighted Ensemble Testing  |     0.8850 |        0.8600 |        0.9100 | 0.7710 | 0.9489 |\n",
      "+----------------------------+------------+---------------+---------------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "best_test_acc = 0.0\n",
    "best_threshold = 0.0\n",
    "\n",
    "# Weights for ensemble \n",
    "weights = np.array(results_df[results_df['Dataset'].str.contains(\"Train\")]['Accuracy'])\n",
    "\n",
    "# Normalize weights to ensure they sum to 1\n",
    "weights /= np.sum(weights)\n",
    "\n",
    "# Define threshold values to test\n",
    "threshold_values = np.arange(0.40, 0.81, 0.001)\n",
    "\n",
    "# Variables to store the best metrics\n",
    "best_train_acc, best_train_sn, best_train_sp, best_train_mcc, best_train_auc = 0, 0, 0, 0, 0\n",
    "best_test_acc, best_test_sn, best_test_sp, best_test_mcc, best_test_auc = 0, 0, 0, 0, 0\n",
    "\n",
    "# Loop through threshold values\n",
    "for threshold in threshold_values:\n",
    "    # Weighted average for training predictions\n",
    "    train_predictions_weighted = np.average(train_predictions_list, axis=0, weights=weights)\n",
    "    train_labels_average = np.array(train_labels_list).mean(axis=0)\n",
    "    \n",
    "    # Evaluate training metrics\n",
    "    train_acc, train_sn, train_sp, train_mcc, train_auc = evaluate_metrics(\n",
    "        train_predictions_weighted, train_labels_average, threshold=threshold\n",
    "    )\n",
    "\n",
    "    # Weighted average for test predictions\n",
    "    test_predictions_weighted = np.average(test_predictions_list, axis=0, weights=weights)\n",
    "    test_labels_average = np.array(test_labels_list).mean(axis=0)\n",
    "\n",
    "    # Evaluate test metrics\n",
    "    test_acc, test_sn, test_sp, test_mcc, test_auc = evaluate_metrics(\n",
    "        test_predictions_weighted, test_labels_average, threshold=threshold\n",
    "    )\n",
    "    \n",
    "    # Check if current threshold yields the best test accuracy or compare MCC when accuracies are equal\n",
    "    if (test_acc > best_test_acc) or (test_acc == best_test_acc and train_acc > best_train_acc):\n",
    "        best_test_acc = test_acc\n",
    "        best_threshold = threshold\n",
    "\n",
    "        # Store the best metrics\n",
    "        best_train_acc, best_train_sn, best_train_sp, best_train_mcc, best_train_auc = train_acc, train_sn, train_sp, train_mcc, train_auc\n",
    "        best_test_acc, best_test_sn, best_test_sp, best_test_mcc, best_test_auc = test_acc, test_sn, test_sp, test_mcc, test_auc\n",
    "\n",
    "# Print the best threshold and corresponding test accuracy\n",
    "print(f\"Best Threshold: {best_threshold:.4f}\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f}\")\n",
    "\n",
    "# Update results table with the best weighted ensemble metrics\n",
    "ensemble_results_weighted = [\n",
    "    [\"Weighted Ensemble Training\", f\"{best_train_acc:.4f}\", f\"{best_train_sn:.4f}\", f\"{best_train_sp:.4f}\", f\"{best_train_mcc:.4f}\", f\"{best_train_auc:.4f}\"],\n",
    "    [\"Weighted Ensemble Testing\", f\"{best_test_acc:.4f}\", f\"{best_test_sn:.4f}\", f\"{best_test_sp:.4f}\", f\"{best_test_mcc:.4f}\", f\"{best_test_auc:.4f}\"]\n",
    "]\n",
    "ensemble_results_df_weighted = pd.DataFrame(ensemble_results_weighted, columns=[\"Dataset\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"MCC\", \"AUC\"])\n",
    "\n",
    "# Display results\n",
    "print(tabulate(ensemble_results_df_weighted, headers=\"keys\", tablefmt=\"grid\", showindex=False, floatfmt=\".4f\"))\n",
    "threshold = best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|   Sample |      3 |      4 |      5 |      6 |   Ensemble |   Label |   Prediction | Correct   |\n",
      "+==========+========+========+========+========+============+=========+==============+===========+\n",
      "|        1 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        2 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        3 | 0.1365 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        4 | 0.1364 | 0.0613 | 0.2417 | 0.7400 |     0.2949 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        5 | 0.8559 | 0.0613 | 0.0750 | 0.2051 |     0.2993 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        6 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        7 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        8 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|        9 | 0.8555 | 0.9276 | 0.9179 | 0.2052 |     0.7266 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       10 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       11 | 0.1364 | 0.9324 | 0.0750 | 0.2051 |     0.3372 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       12 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       13 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       14 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       15 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       16 | 0.1364 | 0.0613 | 0.0752 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       17 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       18 | 0.1676 | 0.0614 | 0.9179 | 0.2063 |     0.3383 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       19 | 0.8560 | 0.9324 | 0.9179 | 0.2079 |     0.7285 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       20 | 0.1365 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       21 | 0.1365 | 0.0613 | 0.2012 | 0.2061 |     0.1513 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       22 | 0.8554 | 0.0614 | 0.1100 | 0.2054 |     0.3081 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       23 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       24 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       25 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       26 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       27 | 0.1365 | 0.0613 | 0.0835 | 0.2052 |     0.1216 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       28 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       29 | 0.1369 | 0.0613 | 0.0750 | 0.2051 |     0.1196 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       30 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       31 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       32 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       33 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       34 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       35 | 0.8555 | 0.9324 | 0.9179 | 0.7866 |     0.8731 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       36 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       37 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       38 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       39 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       40 | 0.2277 | 0.0614 | 0.0764 | 0.2133 |     0.1447 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       41 | 0.1364 | 0.9322 | 0.0750 | 0.2051 |     0.3372 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       42 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       43 | 0.1366 | 0.0614 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       44 | 0.1365 | 0.0864 | 0.9178 | 0.7874 |     0.4820 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       45 | 0.8504 | 0.9323 | 0.9179 | 0.2052 |     0.7265 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       46 | 0.1369 | 0.0715 | 0.0780 | 0.2060 |     0.1231 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       47 | 0.1367 | 0.9324 | 0.0750 | 0.2170 |     0.3403 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       48 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       49 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       50 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       51 | 0.1387 | 0.0613 | 0.0750 | 0.2053 |     0.1201 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       52 | 0.8561 | 0.0637 | 0.9179 | 0.7879 |     0.6564 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       53 | 0.1364 | 0.0615 | 0.0750 | 0.7881 |     0.2652 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       54 | 0.1364 | 0.0613 | 0.0750 | 0.2052 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       55 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       56 | 0.1365 | 0.0613 | 0.9178 | 0.2060 |     0.3304 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       57 | 0.8555 | 0.9318 | 0.0750 | 0.2066 |     0.5172 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       58 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       59 | 0.1370 | 0.0613 | 0.0750 | 0.2051 |     0.1196 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       60 | 0.1364 | 0.0613 | 0.0750 | 0.7880 |     0.2652 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       61 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       62 | 0.1364 | 0.0619 | 0.0750 | 0.2052 |     0.1196 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       63 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       64 | 0.1365 | 0.0615 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       65 | 0.1364 | 0.0613 | 0.0751 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       66 | 0.1364 | 0.0613 | 0.0750 | 0.2062 |     0.1197 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       67 | 0.1434 | 0.0613 | 0.0750 | 0.2051 |     0.1212 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       68 | 0.8559 | 0.0613 | 0.0750 | 0.7881 |     0.4451 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       69 | 0.1366 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       70 | 0.1364 | 0.0619 | 0.0750 | 0.2052 |     0.1196 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       71 | 0.1365 | 0.0613 | 0.9169 | 0.2051 |     0.3300 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       72 | 0.8377 | 0.0614 | 0.2889 | 0.7880 |     0.4940 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       73 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       74 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       75 | 0.1368 | 0.0613 | 0.0750 | 0.2051 |     0.1196 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       76 | 0.8560 | 0.0631 | 0.0750 | 0.7881 |     0.4455 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       77 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       78 | 0.1371 | 0.0614 | 0.0750 | 0.2051 |     0.1197 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       79 | 0.1365 | 0.0613 | 0.2172 | 0.2056 |     0.1551 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       80 | 0.8559 | 0.0625 | 0.0750 | 0.7881 |     0.4454 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       81 | 0.1365 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       82 | 0.1369 | 0.0613 | 0.0750 | 0.2051 |     0.1196 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       83 | 0.1377 | 0.0615 | 0.0750 | 0.2053 |     0.1199 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       84 | 0.1372 | 0.9324 | 0.9179 | 0.7877 |     0.6938 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       85 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       86 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       87 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       88 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       89 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       90 | 0.1365 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       91 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       92 | 0.1365 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       93 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       94 | 0.1370 | 0.0613 | 0.0750 | 0.2051 |     0.1196 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       95 | 0.8559 | 0.8955 | 0.0753 | 0.7881 |     0.6537 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       96 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       97 | 0.8540 | 0.9324 | 0.0750 | 0.2052 |     0.5167 |       0 |            1 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       98 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|       99 | 0.1365 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      100 | 0.1365 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       0 |            0 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      101 | 0.5526 | 0.8740 | 0.9179 | 0.7880 |     0.7831 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      102 | 0.8553 | 0.9324 | 0.9178 | 0.2051 |     0.7277 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      103 | 0.6355 | 0.9324 | 0.9179 | 0.7881 |     0.8184 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      104 | 0.8561 | 0.9319 | 0.9179 | 0.7881 |     0.8735 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      105 | 0.8553 | 0.0613 | 0.0750 | 0.7877 |     0.4448 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      106 | 0.8559 | 0.0613 | 0.0750 | 0.2051 |     0.2993 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      107 | 0.8561 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      108 | 0.8545 | 0.9323 | 0.9177 | 0.7881 |     0.8731 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      109 | 0.1364 | 0.9324 | 0.0750 | 0.2051 |     0.3372 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      110 | 0.8559 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      111 | 0.8561 | 0.9324 | 0.9177 | 0.7880 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      112 | 0.8560 | 0.9324 | 0.9178 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      113 | 0.8561 | 0.9324 | 0.9178 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      114 | 0.8556 | 0.9324 | 0.9179 | 0.7881 |     0.8735 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      115 | 0.8560 | 0.9316 | 0.9178 | 0.7881 |     0.8734 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      116 | 0.8557 | 0.9324 | 0.9179 | 0.7881 |     0.8735 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      117 | 0.8520 | 0.9324 | 0.9179 | 0.7881 |     0.8726 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      118 | 0.8561 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      119 | 0.8557 | 0.9324 | 0.9176 | 0.7879 |     0.8734 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      120 | 0.8559 | 0.9324 | 0.9178 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      121 | 0.8557 | 0.9319 | 0.9179 | 0.7881 |     0.8734 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      122 | 0.1364 | 0.9324 | 0.9178 | 0.7881 |     0.6937 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      123 | 0.1372 | 0.0613 | 0.9175 | 0.2051 |     0.3303 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      124 | 0.1365 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      125 | 0.8561 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      126 | 0.8561 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      127 | 0.1364 | 0.2422 | 0.0750 | 0.2051 |     0.1647 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      128 | 0.8560 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      129 | 0.1365 | 0.9323 | 0.9178 | 0.7845 |     0.6928 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      130 | 0.8555 | 0.9324 | 0.0750 | 0.2051 |     0.5170 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      131 | 0.8560 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      132 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      133 | 0.8561 | 0.9323 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      134 | 0.8560 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      135 | 0.8558 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      136 | 0.8560 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      137 | 0.8561 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      138 | 0.8226 | 0.9324 | 0.9076 | 0.7881 |     0.8627 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      139 | 0.8557 | 0.9324 | 0.9179 | 0.7881 |     0.8735 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      140 | 0.3057 | 0.9324 | 0.0750 | 0.2051 |     0.3796 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      141 | 0.8560 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      142 | 0.8534 | 0.9324 | 0.9179 | 0.7881 |     0.8729 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      143 | 0.8558 | 0.9324 | 0.9178 | 0.7881 |     0.8735 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      144 | 0.8486 | 0.9323 | 0.9179 | 0.7880 |     0.8717 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      145 | 0.8551 | 0.9324 | 0.9179 | 0.7881 |     0.8734 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      146 | 0.5204 | 0.9323 | 0.9179 | 0.7881 |     0.7897 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      147 | 0.8561 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      148 | 0.8560 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      149 | 0.8559 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      150 | 0.8557 | 0.9324 | 0.9179 | 0.7881 |     0.8735 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      151 | 0.8559 | 0.9324 | 0.9179 | 0.7847 |     0.8727 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      152 | 0.8561 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      153 | 0.1365 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      154 | 0.8561 | 0.9323 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      155 | 0.8558 | 0.9324 | 0.9177 | 0.7881 |     0.8735 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      156 | 0.1365 | 0.9321 | 0.9179 | 0.2051 |     0.5479 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      157 | 0.8561 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      158 | 0.1368 | 0.9324 | 0.9179 | 0.7881 |     0.6938 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      159 | 0.8560 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      160 | 0.8560 | 0.9324 | 0.0750 | 0.7881 |     0.6629 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      161 | 0.8551 | 0.9323 | 0.9174 | 0.7881 |     0.8732 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      162 | 0.8561 | 0.9324 | 0.9178 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      163 | 0.1364 | 0.0613 | 0.0750 | 0.2051 |     0.1195 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      164 | 0.8561 | 0.9324 | 0.9178 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      165 | 0.8550 | 0.9324 | 0.9178 | 0.7879 |     0.8733 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      166 | 0.8536 | 0.9324 | 0.6585 | 0.7881 |     0.8081 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      167 | 0.1364 | 0.9319 | 0.0750 | 0.2051 |     0.3371 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      168 | 0.8561 | 0.9324 | 0.9179 | 0.7880 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      169 | 0.8560 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      170 | 0.8560 | 0.9309 | 0.9179 | 0.7881 |     0.8732 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      171 | 0.7931 | 0.9324 | 0.9176 | 0.7881 |     0.8578 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      172 | 0.1364 | 0.9324 | 0.0750 | 0.2051 |     0.3372 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      173 | 0.8561 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      174 | 0.8561 | 0.9324 | 0.9176 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      175 | 0.1364 | 0.0613 | 0.9163 | 0.2051 |     0.3298 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      176 | 0.8559 | 0.0613 | 0.9178 | 0.7881 |     0.6558 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      177 | 0.8553 | 0.9324 | 0.0750 | 0.7881 |     0.6627 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      178 | 0.8560 | 0.9321 | 0.9179 | 0.7881 |     0.8735 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      179 | 0.8560 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      180 | 0.8544 | 0.0613 | 0.9178 | 0.7881 |     0.6554 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      181 | 0.8547 | 0.9268 | 0.9179 | 0.7881 |     0.8719 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      182 | 0.8561 | 0.9323 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      183 | 0.8558 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      184 | 0.1364 | 0.0613 | 0.0750 | 0.7881 |     0.2652 |       1 |            0 | False     |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      185 | 0.8560 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      186 | 0.8561 | 0.9324 | 0.9178 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      187 | 0.8547 | 0.9324 | 0.9179 | 0.7881 |     0.8733 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      188 | 0.8560 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      189 | 0.8556 | 0.9324 | 0.9179 | 0.7881 |     0.8735 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      190 | 0.8561 | 0.9314 | 0.9178 | 0.7881 |     0.8733 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      191 | 0.8555 | 0.9324 | 0.9179 | 0.7881 |     0.8735 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      192 | 0.8555 | 0.8900 | 0.0750 | 0.2051 |     0.5064 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      193 | 0.8560 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      194 | 0.8546 | 0.9324 | 0.9179 | 0.2051 |     0.7275 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      195 | 0.8554 | 0.9324 | 0.9179 | 0.7881 |     0.8734 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      196 | 0.8559 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      197 | 0.8561 | 0.9324 | 0.7190 | 0.7881 |     0.8239 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      198 | 0.8508 | 0.9324 | 0.9179 | 0.7881 |     0.8723 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      199 | 0.8541 | 0.9324 | 0.9177 | 0.7880 |     0.8731 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n",
      "|      200 | 0.8559 | 0.9324 | 0.9179 | 0.7881 |     0.8736 |       1 |            1 | True      |\n",
      "+----------+--------+--------+--------+--------+------------+---------+--------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate individual model scores for each sample\n",
    "individual_model_scores = []\n",
    "for i, kmer in enumerate(kmer_values):\n",
    "    for sample_idx in range(len(test_predictions_list[i])):\n",
    "        individual_model_scores.append({\n",
    "            \"Sample\": sample_idx + 1,\n",
    "            \"k-mer\": kmer,\n",
    "            \"Score\": test_predictions_list[i][sample_idx]\n",
    "        })\n",
    "\n",
    "# Calculate ensemble model scores\n",
    "ensemble_test_predictions = np.array(test_predictions_list).mean(axis=0)\n",
    "for sample_idx in range(len(ensemble_test_predictions)):\n",
    "    individual_model_scores.append({\n",
    "        \"Sample\": sample_idx + 1,\n",
    "        \"k-mer\": \"Ensemble\",\n",
    "        \"Score\": ensemble_test_predictions[sample_idx]\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "individual_model_scores_df = pd.DataFrame(individual_model_scores)\n",
    "\n",
    "# Reshape DataFrame to have each sample as a row, and each model as a column\n",
    "pivot_df = individual_model_scores_df.pivot(index=\"Sample\", columns=\"k-mer\", values=\"Score\")\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "# Adding 'Label' and 'Prediction' columns\n",
    "pivot_df[\"Label\"] = test_labels_list[0]  \n",
    "pivot_df[\"Prediction\"] = (pivot_df[\"Ensemble\"] >= threshold).astype(int)  # Converting ensemble scores to binary predictions\n",
    "\n",
    "# Rename columns to match desired format\n",
    "# Assuming you have 4 k-mer values in kmer_values list\n",
    "column_mapping = {kmer: str(i + 3) for i, kmer in enumerate(kmer_values)}\n",
    "pivot_df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Add 'Correct' column\n",
    "pivot_df[\"Correct\"] = pivot_df[\"Label\"] == pivot_df[\"Prediction\"]\n",
    "\n",
    "# Display the table\n",
    "print(tabulate(pivot_df, headers=\"keys\", tablefmt=\"grid\", showindex=False, floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = round(threshold, 4)\n",
    "print(f\"Threshold: {threshold}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct  False  True \n",
      "Label                \n",
      "0           10     90\n",
      "1           14     86\n",
      "\n",
      "Combined incorrect predictions counts:\n",
      "+---------+-----------------------------------+-----------------------------------+-------+\n",
      "| Model   |   Incorrect Predictions (Label 0) |   Incorrect Predictions (Label 1) |   Sum |\n",
      "+=========+===================================+===================================+=======+\n",
      "| 3       |                                14 |                                16 |    30 |\n",
      "+---------+-----------------------------------+-----------------------------------+-------+\n",
      "| 4       |                                11 |                                12 |    23 |\n",
      "+---------+-----------------------------------+-----------------------------------+-------+\n",
      "| 5       |                                10 |                                16 |    26 |\n",
      "+---------+-----------------------------------+-----------------------------------+-------+\n",
      "| 6       |                                12 |                                17 |    29 |\n",
      "+---------+-----------------------------------+-----------------------------------+-------+\n",
      "| Sum     |                                47 |                                61 |   108 |\n",
      "+---------+-----------------------------------+-----------------------------------+-------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate counts of correct and incorrect predictions for each label\n",
    "label_counts = pivot_df.groupby([\"Label\", \"Correct\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Display the counts\n",
    "print(label_counts)\n",
    "\n",
    "# Calculate incorrect predictions for each k-mer model\n",
    "incorrect_counts = pivot_df[[\"Label\"]].copy()\n",
    "for kmer, col_name in column_mapping.items():\n",
    "    incorrect_counts[col_name] = pivot_df[\"Label\"] != (pivot_df[col_name] >= threshold).astype(int)\n",
    "\n",
    "# Split DataFrame into two separate DataFrames based on labels\n",
    "label_0_df = pivot_df[pivot_df[\"Label\"] == 0]\n",
    "label_1_df = pivot_df[pivot_df[\"Label\"] == 1]\n",
    "\n",
    "# Calculate incorrect predictions for each k-mer model for both labels separately\n",
    "incorrect_counts_label_0 = label_0_df[[\"Label\"]].copy()\n",
    "incorrect_counts_label_1 = label_1_df[[\"Label\"]].copy()\n",
    "\n",
    "for kmer, col_name in column_mapping.items():\n",
    "    incorrect_counts_label_0[col_name] = label_0_df[\"Label\"] != (label_0_df[col_name] >= threshold).astype(int)\n",
    "    incorrect_counts_label_1[col_name] = label_1_df[\"Label\"] != (label_1_df[col_name] >= threshold).astype(int)\n",
    "\n",
    "# Sum incorrect predictions for each k-mer model for both labels separately\n",
    "incorrect_counts_sum_label_0 = incorrect_counts_label_0.sum(axis=0)[1:]\n",
    "incorrect_counts_sum_label_1 = incorrect_counts_label_1.sum(axis=0)[1:]\n",
    "\n",
    "# Combine the incorrect predictions into a single DataFrame\n",
    "combined_incorrect_counts_df = pd.DataFrame({\n",
    "    \"Model\": list(incorrect_counts_sum_label_0.index),\n",
    "    \"Incorrect Predictions (Label 0)\": incorrect_counts_sum_label_0.values,\n",
    "    \"Incorrect Predictions (Label 1)\": incorrect_counts_sum_label_1.values\n",
    "})\n",
    "\n",
    "# Add a sum row at the end\n",
    "sum_row = pd.DataFrame({\n",
    "    \"Model\": [\"Sum\"],\n",
    "    \"Incorrect Predictions (Label 0)\": [incorrect_counts_sum_label_0.sum()],\n",
    "    \"Incorrect Predictions (Label 1)\": [incorrect_counts_sum_label_1.sum()]\n",
    "})\n",
    "\n",
    "combined_incorrect_counts_df = pd.concat([combined_incorrect_counts_df, sum_row], ignore_index=True)\n",
    "\n",
    "# Add a sum column for the rows\n",
    "combined_incorrect_counts_df[\"Sum\"] = combined_incorrect_counts_df[\"Incorrect Predictions (Label 0)\"] + combined_incorrect_counts_df[\"Incorrect Predictions (Label 1)\"]\n",
    "\n",
    "# Display the combined incorrect predictions counts\n",
    "print(\"\\nCombined incorrect predictions counts:\")\n",
    "print(tabulate(combined_incorrect_counts_df, headers=\"keys\", tablefmt=\"grid\", showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import  ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare meta-features for training and testing\n",
    "train_meta_features = np.column_stack(train_predictions_list)\n",
    "test_meta_features = np.column_stack(test_predictions_list)\n",
    "\n",
    "train_labels = train_labels_list[0]     \n",
    "test_labels = test_labels_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+---------------+---------------+--------+--------+\n",
      "| Dataset   |   Accuracy |   Sensitivity |   Specificity |    MCC |    AUC |\n",
      "+===========+============+===============+===============+========+========+\n",
      "| Train     |     1.0000 |        1.0000 |        1.0000 | 1.0000 | 1.0000 |\n",
      "+-----------+------------+---------------+---------------+--------+--------+\n",
      "| Test      |     0.9900 |        1.0000 |        0.9800 | 0.9802 | 0.9900 |\n",
      "+-----------+------------+---------------+---------------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=1000, random_state=42)\n",
    "\n",
    "threshold = 0.9999   \n",
    "\n",
    "# Train the Stacking Classifier on the training set\n",
    "model.fit(train_meta_features, train_labels)\n",
    "\n",
    "# Meta-model predictions on the validation set\n",
    "train_predictions_et = model.predict_proba(train_meta_features)[:, 1]\n",
    "test_predictions_et = model.predict_proba(test_meta_features)[:, 1]\n",
    "\n",
    "train_acc, train_sn, train_sp, train_mcc, train_auc = evaluate_metrics(train_predictions_et, train_labels, threshold=threshold)\n",
    "test_acc, test_sn, test_sp, test_mcc, test_auc = evaluate_metrics(test_predictions_et, test_labels, threshold=threshold)\n",
    "\n",
    "# Display results in table using tabulate\n",
    "results = [[\"Train\", train_acc, train_sn, train_sp, train_mcc, train_auc], [\"Test\", test_acc, test_sn, test_sp, test_mcc, test_auc]]\n",
    "\n",
    "headers = [\"Dataset\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"MCC\", \"AUC\"]\n",
    "\n",
    "print(tabulate(results, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **K-Fold**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5-fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1/5\n",
      "Processing Fold 2/5\n",
      "Processing Fold 3/5\n",
      "Processing Fold 4/5\n",
      "Processing Fold 5/5\n",
      "\n",
      "5-fold cross-validation results:\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|   Fold |    ACC |     SN |     SP |    MCC |    AUC |\n",
      "+========+========+========+========+========+========+\n",
      "|      1 | 0.9125 | 0.8720 | 0.9624 | 0.8298 | 0.9575 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      2 | 0.9663 | 0.9403 | 0.9877 | 0.9325 | 0.9925 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      3 | 0.9293 | 0.9220 | 0.9359 | 0.8582 | 0.9666 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      4 | 0.9360 | 0.8865 | 0.9808 | 0.8746 | 0.9702 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      5 | 0.9426 | 0.9506 | 0.9328 | 0.8840 | 0.9804 |\n",
      "+--------+--------+--------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Parameters for k-fold\n",
    "k_folds = 5  # Number of folds\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "threshold = 0.5\n",
    "\n",
    "# Initialize results storage\n",
    "kfold_results = []\n",
    "\n",
    "# Convert meta-features and labels to numpy arrays for k-fold processing\n",
    "train_meta_features = np.array(train_meta_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_meta_features)):\n",
    "    print(f\"Processing Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    # Split data into training and validation sets for this fold\n",
    "    X_train, X_val = train_meta_features[train_idx], train_meta_features[val_idx]\n",
    "    y_train, y_val = train_labels[train_idx], train_labels[val_idx]\n",
    "\n",
    "    # Initialize and train Extra Trees model\n",
    "    model = ExtraTreesClassifier(n_estimators=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions on validation set\n",
    "    val_predictions = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Evaluate metrics \n",
    "    val_acc, val_sn, val_sp, val_mcc, val_auc = evaluate_metrics(val_predictions, y_val, threshold=threshold)\n",
    "\n",
    "    # Store results for this fold\n",
    "    kfold_results.append({\"Fold\": fold + 1, \"ACC\": val_acc, \"SN\": val_sn, \"SP\": val_sp, \"MCC\": val_mcc, \"AUC\": val_auc})\n",
    "\n",
    "# Display aggregated results across folds\n",
    "print(f'\\n{k_folds}-fold cross-validation results:')\n",
    "headers = [\"Fold\", \"ACC\", \"SN\", \"SP\", \"MCC\", \"AUC\"]\n",
    "results_table = [[result[\"Fold\"], result[\"ACC\"], result[\"SN\"], result[\"SP\"], result[\"MCC\"], result[\"AUC\"]] for result in kfold_results]\n",
    "print(tabulate(results_table, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 5-fold cross-validation results:\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "| Fold   |    ACC |     SN |     SP |    MCC |    AUC |\n",
      "+========+========+========+========+========+========+\n",
      "| Mean   | 0.9373 | 0.9143 | 0.9599 | 0.8758 | 0.9734 |\n",
      "+--------+--------+--------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display average results\n",
    "avg_acc = np.mean([result[\"ACC\"] for result in kfold_results])\n",
    "avg_sn = np.mean([result[\"SN\"] for result in kfold_results])\n",
    "avg_sp = np.mean([result[\"SP\"] for result in kfold_results])\n",
    "avg_mcc = np.mean([result[\"MCC\"] for result in kfold_results])\n",
    "avg_auc = np.mean([result[\"AUC\"] for result in kfold_results])\n",
    "\n",
    "print(f'Average {k_folds}-fold cross-validation results:')\n",
    "avg_results_table = [[\"Mean\", avg_acc, avg_sn, avg_sp, avg_mcc, avg_auc]]\n",
    "print(tabulate(avg_results_table, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10-fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1/10\n",
      "Processing Fold 2/10\n",
      "Processing Fold 3/10\n",
      "Processing Fold 4/10\n",
      "Processing Fold 5/10\n",
      "Processing Fold 6/10\n",
      "Processing Fold 7/10\n",
      "Processing Fold 8/10\n",
      "Processing Fold 9/10\n",
      "Processing Fold 10/10\n",
      "\n",
      "10-fold cross-validation results:\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|   Fold |    ACC |     SN |     SP |    MCC |    AUC |\n",
      "+========+========+========+========+========+========+\n",
      "|      1 | 0.9396 | 0.9250 | 0.9565 | 0.8796 | 0.9671 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      2 | 0.8792 | 0.8214 | 0.9538 | 0.7694 | 0.9525 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      3 | 0.9732 | 0.9375 | 1.0000 | 0.9462 | 0.9858 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      4 | 0.9664 | 0.9583 | 0.9740 | 0.9329 | 0.9896 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      5 | 0.9257 | 0.8889 | 0.9529 | 0.8478 | 0.9595 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      6 | 0.9392 | 0.9605 | 0.9167 | 0.8789 | 0.9754 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      7 | 0.9595 | 0.9231 | 0.9880 | 0.9186 | 0.9653 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      8 | 0.9189 | 0.8553 | 0.9861 | 0.8460 | 0.9715 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|      9 | 0.9459 | 0.9529 | 0.9365 | 0.8894 | 0.9887 |\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "|     10 | 0.9392 | 0.9481 | 0.9296 | 0.8782 | 0.9641 |\n",
      "+--------+--------+--------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Parameters for k-fold\n",
    "k_folds = 10  # Number of folds\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "threshold = 0.5\n",
    "\n",
    "# Initialize results storage\n",
    "kfold_results = []\n",
    "\n",
    "# Convert meta-features and labels to numpy arrays for k-fold processing\n",
    "train_meta_features = np.array(train_meta_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_meta_features)):\n",
    "    print(f\"Processing Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    # Split data into training and validation sets for this fold\n",
    "    X_train, X_val = train_meta_features[train_idx], train_meta_features[val_idx]\n",
    "    y_train, y_val = train_labels[train_idx], train_labels[val_idx]\n",
    "\n",
    "    # Initialize and train Extra Trees model\n",
    "    model = ExtraTreesClassifier(n_estimators=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions on validation set\n",
    "    val_predictions = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Evaluate metrics \n",
    "    val_acc, val_sn, val_sp, val_mcc, val_auc = evaluate_metrics(val_predictions, y_val, threshold=threshold)\n",
    "\n",
    "    # Store results for this fold\n",
    "    kfold_results.append({\"Fold\": fold + 1, \"ACC\": val_acc, \"SN\": val_sn, \"SP\": val_sp, \"MCC\": val_mcc, \"AUC\": val_auc})\n",
    "\n",
    "# Display aggregated results across folds\n",
    "print(f'\\n{k_folds}-fold cross-validation results:')\n",
    "headers = [\"Fold\", \"ACC\", \"SN\", \"SP\", \"MCC\", \"AUC\"]\n",
    "results_table = [[result[\"Fold\"], result[\"ACC\"], result[\"SN\"], result[\"SP\"], result[\"MCC\"], result[\"AUC\"]] for result in kfold_results]\n",
    "print(tabulate(results_table, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 10-fold cross-validation results:\n",
      "+--------+--------+--------+--------+--------+--------+\n",
      "| Fold   |    ACC |     SN |     SP |    MCC |    AUC |\n",
      "+========+========+========+========+========+========+\n",
      "| Mean   | 0.9387 | 0.9171 | 0.9594 | 0.8787 | 0.9719 |\n",
      "+--------+--------+--------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display average results\n",
    "avg_acc = np.mean([result[\"ACC\"] for result in kfold_results])\n",
    "avg_sn = np.mean([result[\"SN\"] for result in kfold_results])\n",
    "avg_sp = np.mean([result[\"SP\"] for result in kfold_results])\n",
    "avg_mcc = np.mean([result[\"MCC\"] for result in kfold_results])\n",
    "avg_auc = np.mean([result[\"AUC\"] for result in kfold_results])\n",
    "\n",
    "print(f'Average {k_folds}-fold cross-validation results:')\n",
    "avg_results_table = [[\"Mean\", avg_acc, avg_sn, avg_sp, avg_mcc, avg_auc]]\n",
    "print(tabulate(avg_results_table, headers=headers, tablefmt=\"grid\", floatfmt=\".4f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
